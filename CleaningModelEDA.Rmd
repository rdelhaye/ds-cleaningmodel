---
title: "Cleaning Data EDA"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
---

```{r, include=FALSE}
library(data.table)
library(ggplot2)

Sys.setenv(R_CONFIG_ACTIVE = Sys.info()['user'])
setwd(config::get('wd'))

```

# Introduction

The purpose of this notebook is to front the analysis of sample data, observed from the outflow of a manufacturing site, in order to determine how to qualify product loss. The data were provided by the stakeholders, and are stored in a CSV file in the `Data/` directory. Note that as this analysis is hosted in a public Git repository, both the CSV and accompanying PDF are excluded from the repo itself, as the IP does not belong to the author.

# Provided Data

Time series sampled at 60 second intervals of:

* Outflow rate, normalised. 
* Outflow turbidity, normalised.

# User Story, Problem Statement

As a stakeholder, I need insight into the quantity lost per day, so that I can invoke remedial action the following day and minimise our losses. However, I do not have a clear concept of what "typical" or "poor" losses look like.

# Assumptions

* Given the dimensionless data provided, we must assume the most basic inference that the flow rate is in units per minute.
* May not be relevant, but, assuming that flow rate is linear, laminar, and appropriately handled before normalisation.
* There's an explicit assumption that turbidity correlates "directly" with the proportion lost. I would question the implication that 100% turbidity equates 100% product (my suspicion is that the turbidity limit is reached before 100% loss); I am hence going to define a correlation value between 0 and 1 that represents the proportion of product within the flow when turbidity is 100%, to give us a lever for later adjustment if required. 

```{r}
turbidity.loss.coeff <- 1.0
```

# Clarifying Questions

For discussion with stakeholders:

* Did something occur in July that affected the flow sensor?
* SImilarly, we have dramatically more missing flow data than loss data. I would query why.

# Data Loading
```{r}
in.fn <- 'Data/interview_data.csv'
input.df <- fread(in.fn)

```

# Data Quality

Data comprised of three columns, namely, timestamps, normalised loss measures, and normalised flow measures.

## Timestamp quality
```{r}
min(input.df$timestamp)
max(input.df$timestamp)

table(diff(input.df$timestamp))
```

First check is to verify how regularly sampled the data are. Of the 494,079 rows, 32 have time differences greater than one minute, with the worst gap of 5,817 minutes - just under 97 hours.

The gaps of six minutes or less may indicate issues in the sampling method, power interruptions etc; interpolation across gaps may be acceptable.

The longer duration gaps of 29 minutes up to and past hours require more checking - are the outages expected and/or explainable, was the site operational during this time or closed etc? Given the sampling period is just under one calendar year, it could be that these gaps coincide with major festivals or holiday periods.

The dates and times at which these occurred:
```{r}
# Get something in here assessing what the start and end times of the longer duration gaps were, see if there's a pattern.
input.df[,`:=`(prev.time = shift(timestamp),prev.flow = shift(normalised_flow))][,time.delta := as.numeric(difftime(timestamp,prev.time,units='mins'))]
input.df[time.delta>20,.(prev.time,timestamp, time.delta,normalised_flow,prev.flow)]
```

## Loss quality
```{r}
sum(is.na(input.df$normalised_losses))
min(input.df$normalised_losses,na.rm=TRUE)
max(input.df$normalised_losses,na.rm=TRUE)
mean(input.df$normalised_losses,na.rm=TRUE)
median(input.df$normalised_losses,na.rm=TRUE)
```

We are missing 158 observations of loss. The remainder are appropriately scaled between 0 and 1, with measures of central tendency falling close to 0.3. A mean slightly higher than the median suggesting a longer tail towards higher values (i.e., there are definitely some times of higher loss).

### Histogram
```{r}
hist(input.df$normalised_losses, breaks=20)
plot(seq(20),log10(hist(input.df$normalised_losses, breaks=20,plot=FALSE)$counts))
```

The vast majority of normalised loss values fall around 0.3. However, a disproportionately high 4% of loss values approach 100%, and do not conform to the expected distribution behaviour. Clearly, significant outliers to be evaluated.


## Flow quality
```{r}
sum(is.na(input.df$normalised_flow))
min(input.df$normalised_flow,na.rm=TRUE)
max(input.df$normalised_flow,na.rm=TRUE)
mean(input.df$normalised_flow,na.rm=TRUE)
median(input.df$normalised_flow,na.rm=TRUE)
```

We are missing 37,210 observations of flow. Whilst this represents 7% of the total rows, due to the loss being measured as a proportion of flow this impacts the data available more significantly. Possible remedial actions:
* Imputation. Given that we lacked only 158 measures of loss, the remaining 37,052 flow points cannot be assumed as 0. Depending on the intervals of missing data, and establishing if missing at random of not, we may be able to impute with a linear or otherwise model (particularly if there are distinct patterns or waveforms in the flow data with time), or alternately with a **k**-means approach. It would be nice to have additional, non-target covariates to support this.
* Alternately, if the problem is determined to be time-invariant (i.e., losses may worsen at any point in cleaning, not due to a certain process), the values may be omitted. Root cause analysis still advised, as data go missing for reasons.


# Analysis

## Naive histograms

Histograms of the provided quantities show reasonable distributions, with some long tails towards right, especially for losses.

Due to the integral nature of the situation being considered, where we have a total quantity and a proportion thereof, we are introducing the product of the two provided time series as `quantity`, reflecting the normalised units per unit time lost. 
```{r}

hist(input.df$normalised_losses)
# Big spike of high losses that might be explainable, but clearly outliers = need to be validated.
hist(input.df$normalised_flow)
# Flow, on the other hand, looks fairly reasonably distributed.

# There's something in the integral quality to be considered, i.e., the product of loss * flow, as high loss at minimal flow has small absolute value.
hist(log10(input.df$normalised_losses*input.df$normalised_flow))
input.df$normalised_quantity <- input.df$normalised_losses*input.df$normalised_flow*turbidity.loss.coeff
input.df$lost_quantity <- input.df$normalised_quantity*input.df$time.delta
# input.df[,loss_group:=fifelse(log10(normalised_quantity)>-3,2,1)]
# table(input.df$loss_group)

```

There exists a large quantity of high losses that might be explainable, but clearly, as outliers these need to be validated. Flow looks reasonably distributed.

Given that the end goal of this analysis likely ends up requiring a $ value against a quantity of lost product, and that the quantity of product would be computed from the... product... of loss and flow rate, a histogram of the `log10(loss * flow)` has also been considered. The values comprising this distribution represent units of product lost per time step, where the units and time step come from the dimensional analysis of the flow rate (e.g., if flow is measured in litres per second, that carries through to the product). Two clear populations of product are evident in this histogram.

## Temporal Patterns

For the sake of inferring trends over time, we want to recast the date and timestamps, at various levels, as sin and cosine pairs (to accurately capture cyclic behaviour). 
```{r}
radial.plt.df <- copy(input.df)
# month,
radial.plt.df$month.sin <- sinpi(2*as.numeric(format(radial.plt.df$timestamp,format= "%j"))/365)
radial.plt.df$month.cos <- cospi(2*as.numeric(format(radial.plt.df$timestamp,format= "%j"))/365)


# Day of week
radial.plt.df$DoW.sin <- sinpi(2*(1+as.numeric(format(radial.plt.df$timestamp,format= "%w")))/8)
radial.plt.df$Dow.cos <- cospi(2*(1+as.numeric(format(radial.plt.df$timestamp,format= "%w")))/8)

# Time of day (minutes? Decimal hours?)
radial.plt.df$ToD.sin <- sinpi(2*(as.numeric(format((radial.plt.df$timestamp),format= "%H"))+as.numeric(format((radial.plt.df$timestamp),format= "%M"))/60)/24)
radial.plt.df$ToD.cos <- cospi(2*(as.numeric(format((radial.plt.df$timestamp),format= "%H"))+as.numeric(format((radial.plt.df$timestamp),format= "%M"))/60)/24)
```

## Radial Heatmaps
Using the sin/cos terms, we can visualise the cyclic distribution of loss, flow, and their product (quantity hereafter). This is a little fancier than it is useful, sadly. Didn't quite plot how I wanted, but still an interesting exercise. To interpret the rose plots, the 0 mark (midnight, Sunday into Monday, start of year etc) is at 12 o'clock, and time progresses clockwise. The radial distance from the origin gives the measure.

### Time of Day behaviour
```{r}

radial.plt.df$theta.ToD <- atan2(radial.plt.df$ToD.sin, radial.plt.df$ToD.cos) * (180 / pi)
radial.plt.df$theta.ToD[radial.plt.df$theta.ToD < 0] <- radial.plt.df$theta.ToD[radial.plt.df$theta.ToD < 0] + 360
# 
# num_bins <- 50  # Or however many bins you want
# radial.plt.df$theta_bin <- cut(radial.plt.df$theta, breaks = num_bins, labels = FALSE)
# radial.plt.df$loss_bin <- cut(radial.plt.df$normalised_losses, breaks = num_bins, labels = FALSE)
# radial.plt.df$flow_bin <- cut(radial.plt.df$normalised_flow, breaks = num_bins, labels = FALSE)

ToD.radial.plt.F <- ggplot(data = radial.plt.df)+
  geom_hex(aes(x=theta.ToD,y=normalised_flow)) +
  coord_polar(theta = 'x')+
  scale_fill_gradient(trans='log1p' )
ToD.radial.plt.F

ToD.radial.plt.L <- ggplot(data = radial.plt.df)+
  geom_hex(aes(x=theta.ToD,y=normalised_losses)) +
  coord_polar(theta = 'x')+
  scale_fill_gradient(trans='log1p' )
ToD.radial.plt.L
```

Both flow and loss look fairly steady throughout time of day variation.

### Day of year
```{r}
# 
radial.plt.df$theta.DoY <- atan2(radial.plt.df$month.sin, radial.plt.df$month.cos) * (180 / pi)
radial.plt.df$theta.DoY[radial.plt.df$theta.DoY < 0] <- radial.plt.df$theta.DoY[radial.plt.df$theta.DoY < 0] + 360

Month.radial.plt.F <- ggplot(data = radial.plt.df)+
  geom_hex(aes(x=theta.DoY,y=normalised_flow)) +
  coord_polar(theta = 'x')+
  scale_fill_gradient(trans='log1p' )
Month.radial.plt.F

Month.radial.plt.L <- ggplot(data = radial.plt.df)+
  geom_hex(aes(x=theta.DoY,y=normalised_losses)) +
  coord_polar(theta = 'x')+
  scale_fill_gradient(trans='log1p' )
Month.radial.plt.L
```

The flow rates with respect to decimal day of year are steady. However, the loss quantities show a strong outlying sector of time that should be examined further. Reverting to a rectilinear histogram:
```{r}
radial.plt.df$DoY <- format(radial.plt.df$timestamp,format= "%j")
radial.plt.df$month <- format(radial.plt.df$timestamp,format= "%m")

DoY.rect.plt.L <- ggplot(data = radial.plt.df)+
  geom_hex(aes(x=DoY,y=normalised_losses)) +
  scale_fill_gradient(trans='log1p' )
DoY.rect.plt.L

hist(unlist(radial.plt.df[as.numeric(month)<10,.(normalised_losses)]),breaks=20)
hist(unlist(radial.plt.df[as.numeric(month)>10,.(normalised_losses)]),breaks=20)
```

So, data in November and December have markedly different loss behaviours than other times. Given that the flow data are stable with respect to other time periods, and it is only the loss data that are affected, there may be a source of additional noise, or perhaps a change in systems or workforce. Irrespective of the source, it would be inappropriate to , and we could consider either two models for Nov/Dec-times and not, or a model with appropriate indicator variable handling to model both. Alternately, 

### Day of Week

```{r,eval=FALSE}
# 
radial.plt.df$theta.DoW <- atan2(radial.plt.df$DoW.sin, radial.plt.df$Dow.cos) * (180 / pi)
radial.plt.df$theta.DoW[radial.plt.df$theta.DoW < 0] <- radial.plt.df$theta.DoW[radial.plt.df$theta.DoW < 0] + 360
# radial.plt.df$theta_bin <- cut(radial.plt.df$theta, breaks = num_bins, labels = FALSE)

DoW.radial.plt.F <- ggplot(data = radial.plt.df)+
  geom_hex(aes(x=theta.DoW,y=normalised_flow)) +
  coord_polar(theta = 'x')+
  scale_fill_gradient(trans='log1p' )
DoW.radial.plt.F

DoW.radial.plt.L <- ggplot(data = radial.plt.df)+
  geom_hex(aes(x=theta.DoW,y=normalised_losses)) +
  coord_polar(theta = 'x')+
  scale_fill_gradient(trans='log1p' )
DoW.radial.plt.L
```

# Flow-Loss Relationship Modelling, ex Nov/Dec


As mentioned previously, in order to quantify the amount of product lost, we must consider the product of the flow and loss rates. However, there are several points to consider:

* Non-uniform timesteps (i.e., missing records). Two options, either interpolate across the time gaps with appropriate imputation, or assume that 0 flow occured in the time frame. Imputation would be preferable where the gap is a small number of timesteps, however, the few multiple-day records should not be considered.
* Nov/Dec loss anomaly. Many options, including omitting the months outright, handling with indicator variables, or a separate model. For the MVP process, we will omit the Nov/Dec data in favour of a general time-invariant solution.
* Missing loss/flow measurements. Imputation should be sufficient. 

Given the current paucity of features, imputation with K-means is unlikely to be much improvement over linear interpolation, or median imputation. User familiarity is with the `caret` package for imputation.

## Imputation of data

```{r}
library(caret)
caret.input.df <- copy(input.df)
preproc.vals <- caret::preProcess(input.df,method=c('medianImpute'))
imputed.df <- stats::predict(preproc.vals,caret.input.df)
```

## Aggregation by Day

```{r}
imputed.df$cal.date <- as.Date(imputed.df$timestamp)
imputed.day.dt <- unique(imputed.df[,`:=`(total.lost = sum(lost_quantity,na.rm=TRUE)),by=cal.date][,.(cal.date, total.lost)])
plot(imputed.day.dt$cal.date,imputed.day.dt$total.lost)
imputed.day.dt[,DoD.variation := total.lost-shift(total.lost)][,DoD.var.relative := DoD.variation/total.lost][abs(DoD.var.relative)>100,DoD.var.relative:=NA]
plot(tail(imputed.day.dt$cal.date,n=-1),diff(imputed.day.dt$total.lost))
```

A quick plot of total lost per day shows three anomalous periods: the aforementioned December period (highly elevated quantities from 23/11/2022 to 21/12/2022), but also recent late May (reduced quantities from 16/05/23 to 09/06/2023), and from mid-June (commencing 10/06). This raises a conundrum about what "normal" looks like, as for the last two months, the experiment site has been anomalous.

Given the brief, we interpret the ask as "What does normal look like?", so that we can establish a context to justify claims of non-normality.

## Subset of Data to Evaluate
```{r}
subset.dt <- copy(imputed.day.dt)[!(cal.date>="2022-11-23" & cal.date<="2022-12-21"),][!(cal.date>="2023-05-16" & cal.date<="2023-06-09"),][!(cal.date>="2023-06-10"),]
hist(subset.dt$total.lost)
```

## Distribution of Total Daily Loss

```{r}
shapiro.test(subset.dt$total.lost)
ks.test(subset.dt$total.lost,'pnorm')

paste0('Median: ',median(subset.dt$total.lost))
paste0('MAD: ',mad(subset.dt$total.lost))
```
Verifying with both Shapiro-Wilks and Kolmogorov-Smirnoff, the total daily losses do not quite follow a normal distribution. In order to answer the business question, we still require an estimation of the bounds of routine; to maintain statistical robustness, as an MVP solution I propose we use the sample median and median absolute deviation (MAD) to establish Red/Amber/Green traffic-light criteria, where Green represents within 1 MAD of the median (72% of the subset data), Amber represents within 2 MAD of the median (95% of the subset data), and Red is greater than 2 MAD.

Numerically, this looks like the following hierarchy:

* Total losses between 39.5 and 54.8 units: Green.
* Total losses between 31.9 and 62.4 units: Amber.
* Otherwise, losses outside typical behaviour: Red.

# Application of Solution

MVP 1 is a straightforward comparison of total loss in a day to the defined RAG thresholds, as proscribed in the below function `fn_lossRag()`:
```{r}
fn_lossRag <- function(total.day.loss=NULL,dist.median = 47.14, dist.mad = 7.62){
  ## Function to validate whether a day's losses are within usual parameters.
  # distribution parameters can be passed as an argument
  
  ## INPUT:
  # total.day.loss - Numeric value of total units of product estimated lost within a day/24 hour period.
  
  ## OUTPUT:
  # RAG.value - Text string of 'Green', 'Amber', or 'Red' depending on hardcoded bounds.
  
  RAG.value <- switch(min(ceiling( abs(total.day.loss-dist.median)/dist.mad),3),'GREEN','AMBER','RED')
  RAG.value
}

```

```{r}
full.daily.RAG <- do.call('rbind',lapply(imputed.day.dt$total.lost,fn_lossRag))
table(full.daily.RAG)
```

It should be noted that as coded, the MVP solution assumes that the system behaviour is at a steady state; given the recent values it may not be, and the reference distribution values updated accordingly. The approach would be inappropriate in the presence of incomplete, non-imputable data.

Additionally, although the majority of product loss appears to occur within the first 12 hours of a given day, it should be noted that this approach implicitly infers a workflow wherein the comparison is made at the end of the day; for business reasons it may be more practicable to have an indicator available closer to COB times.

## Further development of solution

An extension of the above method would be to robustly model the trajectory of loss accumulation as a function of time for each day within the sample data. This would allow extrapolation from measurements through only part of the day and estimate the total at end of day, before losses actualised. This approach would solve the business requirement to inform next day activity.