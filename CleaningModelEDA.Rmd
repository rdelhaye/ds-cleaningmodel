---
title: "Cleaning Data EDA"
output: html_notebook
---

```{r, include=FALSE}
library(data.table)
library(ggplot2)

Sys.setenv(R_CONFIG_ACTIVE = Sys.info()['user'])
setwd(config::get('wd'))

```

# Introduction

The purpose of this notebook is to front the analysis of sample data, observed from the outflow of a manufacturing site, in order to determine how to qualify product loss. The data were provided by the stakeholders, and are stored in a CSV file in the `Data/` directory. Note that as this analysis is hosted in a public Git repository, both the CSV and accompanying PDF are excluded from the repo itself, as the IP does not belong to the author.

# Provided Data

# User Story, Problem Statement

# Assumptions

# Data Loading
```{r}
in.fn <- 'Data/interview_data.csv'
source('R_Code/01_DataReading.R')
```

# Data Quality

Data comprised of three columns, namely, timestamps, normalised loss measures, and normalised flow measures.

## Timestamp quality
```{r}
min(input.df$timestamp)
max(input.df$timestamp)

table(diff(input.df$timestamp))
```

First check is to verify how regularly sampled the data are. Of the 494,079 rows, 32 have time differences greater than one minute, with the worst gap of 5,817 minutes - just under 97 hours.

The gaps of six minutes or less may indicate issues in the sampling method, power interruptions etc; interpolation across gaps may be acceptable.

The longer duration gaps of 29 minutes up to and past hours require more checking - are the outages expected and/or explainable, was the site operational during this time or closed etc? Given the sampling period is just under one calendar year, it could be that these gaps coincide with major festivals or holiday periods.

The dates and times at which these occurred:
```{r}
# Get something in here assessing what the start and end times of the longer duration gaps were, see if there's a pattern.
input.df[,prev.time := shift(timestamp)][,time.delta := as.numeric(difftime(timestamp,prev.time,units='mins'))]
input.df[time.delta>20,.(timestamp, normalised_losses,normalised_flow,time.delta)]
```

## Loss quality
```{r}
sum(is.na(input.df$normalised_losses))
min(input.df$normalised_losses,na.rm=TRUE)
max(input.df$normalised_losses,na.rm=TRUE)
mean(input.df$normalised_losses,na.rm=TRUE)
median(input.df$normalised_losses,na.rm=TRUE)
```

We are missing 158 observations of loss. The remainder are appropriately scaled between 0 and 1, with measures of central tendency falling close to 0.3. A mean slightly higher than the median suggesting a longer tail towards higher values (i.e., there are definitely some times of higher loss).

### Histogram

## Flow quality
```{r}
sum(is.na(input.df$normalised_flow))
min(input.df$normalised_flow,na.rm=TRUE)
max(input.df$normalised_flow,na.rm=TRUE)
mean(input.df$normalised_flow,na.rm=TRUE)
median(input.df$normalised_flow,na.rm=TRUE)
```

We are missing 37,210 observations of flow. Whilst this represents 7% of the total rows, due to the loss being measured as a proportion of flow this impacts the data available more significantly. Possible remedial actions:
* Imputation. Given that we lacked only 158 measures of loss, the remaining 37,052 flow points cannot be assumed as 0. Depending on the intervals of missing data, and establishing if missing at random of not, we may be able to impute with a linear or otherwise model (particularly if there are distinct patterns or waveforms in the flow data with time), or alternately with a **k**-means approach. It would be nice to have additional, non-target covariates to support this.
* Alternately, if the problem is determined to be time-invariant (i.e., losses may worsen at any point in cleaning, not due to a certain process), the values may be omitted. Root cause analysis still advised, as data go missing for reasons.


# Analysis

## Naive histograms

Histograms of the provided quantities show reasonable distributions, with some long tails towards right, especially for losses.

Due to the integral nature of the situation being considered, where we have a total quantity and a proportion thereof, we are introducing the product of the two provided time series as `quantity`, reflecting the normalised units per unit time lost. 
```{r}

hist(input.df$normalised_losses)
# Big spike of high losses that might be explainable, but clearly outliers = need to be validated.
hist(input.df$normalised_flow)
# Flow, on the other hand, looks fairly reasonably distributed.

# There's something in the integral quality to be considered, i.e., the product of loss * flow, as high loss at minimal flow has small absolute value.
hist(log10(input.df$normalised_losses*input.df$normalised_flow))
input.df$normalised_quantity <- input.df$normalised_losses*input.df$normalised_flow
input.df$lost_quantity <- input.df$normalised_quantity*input.df$time.delta
input.df[,loss_group:=fifelse(log10(normalised_quantity)>-3,2,1)]
table(input.df$loss_group)

```

There exists a large quantity of high losses that might be explainable, but clearly, as outliers these need to be validated. Flow looks reasonably distributed.

Given that the end goal of this analysis likely ends up requiring a $ value against a quantity of lost product, and that the quantity of product would be computed from the... product... of loss and flow rate, a histogram of the `log10(loss * flow)` has also been considered. The values comprising this distribution represent units of product lost per time step, where the units and time step come from the dimensional analysis of the flow rate (e.g., if flow is measured in litres per second, that carries through to the product). Two clear populations of product are evident in this histogram.

## Temporal Patterns

For the sake of inferring trends over time, we want to recast the date and timestamps, at various levels, as sin and cosine pairs (to accurately capture cyclic behaviour). 
```{r}
# month,
input.df$month.sin <- sinpi(2*as.numeric(format(input.df$timestamp,format= "%m"))/12)
input.df$month.cos <- cospi(2*as.numeric(format(input.df$timestamp,format= "%m"))/12)

# Day of week
input.df$DoW.sin <- sinpi(2*as.numeric(format(input.df$timestamp,format= "%w"))/7)
input.df$Dow.cos <- cospi(2*as.numeric(format(input.df$timestamp,format= "%w"))/7)

# Time of day (minutes? Decimal hours?)
input.df$ToD.sin <- sinpi(2*(as.numeric(format((input.df$timestamp),format= "%H"))+as.numeric(format((input.df$timestamp),format= "%M"))/60)/24)
input.df$ToD.cos <- cospi(2*(as.numeric(format((input.df$timestamp),format= "%H"))+as.numeric(format((input.df$timestamp),format= "%M"))/60)/24)
```

## Radial Heatmaps
Using the sin/cos terms, we can visualise the cyclic distribution of loss, flow, and their product (quantity hereafter). This is a little fancier than it is useful, sadly. Didn't quite plot how I wanted, but still an interesting exercise. 
```{r}
radial.plt.df <- copy(input.df)
radial.plt.df$theta <- atan2(radial.plt.df$ToD.sin, radial.plt.df$ToD.cos) * (180 / pi)
radial.plt.df$theta[radial.plt.df$theta < 0] <- radial.plt.df$theta[radial.plt.df$theta < 0] + 360

num_bins <- 50  # Or however many bins you want
radial.plt.df$theta_bin <- cut(radial.plt.df$theta, breaks = num_bins, labels = FALSE)
radial.plt.df$loss_bin <- cut(radial.plt.df$normalised_losses, breaks = num_bins, labels = FALSE)

ToD.radial.plt <- ggplot(data = radial.plt.df)+
  geom_hex(aes(x=theta,y=normalised_losses)) +
  coord_polar(theta = 'x')+
  scale_fill_gradient(trans='log1p' )
ToD.radial.plt

radial.plt.df$theta <- atan2(radial.plt.df$month.sin, radial.plt.df$month.cos) * (180 / pi)
radial.plt.df$theta_bin <- cut(radial.plt.df$theta, breaks = 12, labels = FALSE)

Month.radial.plt <- ggplot(data = radial.plt.df)+
  geom_hex(aes(x=theta,y=normalised_losses)) +
  coord_polar(theta = 'x')+
  scale_fill_gradient(trans='log1p' )
Month.radial.plt

radial.plt.df$theta <- atan2(radial.plt.df$DoW.sin, radial.plt.df$Dow.cos) * (180 / pi)
radial.plt.df$theta_bin <- cut(radial.plt.df$theta, breaks = num_bins, labels = FALSE)

DoW.radial.plt <- ggplot(data = radial.plt.df)+
  geom_hex(aes(x=theta,y=normalised_losses)) +
  coord_polar(theta = 'x')+
  scale_fill_gradient(trans='log1p' )
DoW.radial.plt

```

## Thought: Cleaning processes
I wonder if we might be able to establish the waveforms that occur as part of a clean-down process, enabling us to compare normalised waveforms across events?